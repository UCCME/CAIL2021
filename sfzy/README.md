# CAIL2021 —— 司法摘要

该项目为 **CAIL2021——司法摘要** 的代码和模型提交说明

## 任务介绍
该赛道由**擎盾数据**与**中国科学院自动化研究所模式识别国家重点实验室**承办。法律领域回答信息来源较多，领域丰富，绝大多数回答片面，语句冗长，这给司法智能问答带来了挑战。融合多来源答案数据来源丰富，同时融合多文档信息进行摘要精简，可以极大提高答案的质量，从而提升智能问答与文档摘要领域的研究水平，对我国在“互联网＋”和“大数据”时代的法治中国建设起到推进作用。

具体来说，我们会提供法律咨询的用户提问和若干个回答，作为真实的问答数据集，选手的任务是输出对应的正确、完整、简洁的参考回答。允许选手使用任何外部知识来帮助模型，但是我们要求选手在预测过程中不能够进行联网的操作。

## 数据集说明

本任务技术评测使用的训练集、验证集、测试集来自互联网，包含大约24000条问题，每条问题包含2~4条律师解答以及对应的摘要后的标准答案。

数据均包含若干行，每行数据为json格式，包含若干字段：

| 字段名     |     说明 |
| :---------- | --------:|
| id          |   文档编号 |
| question    |   法律问题 |
| candidates  |  参考答案  |
| answer      |   人工摘要答案 |

实际测试数据不包含``answer``

## 提交的文件格式

你可以在`python_sample`中找到最简单的提交代码的格式。你需要将你所有的代码压缩为一个`zip`文件进行提交，该`zip`文件内部形式可以参看`python_sample/main.zip`。该`zip`文件**内部顶层**必须包含`main.py`，为运行的入口程序，我们会在该目录下使用`python3 main.py --input INPUT_PATH --output OUTPUT_PATH`来运行你的程序。在正式测试程序时，后端脚本将自动分配合适的参数路径，因此你无需关心`--input`和`--output`参数。

## 代码的内容

对于你的代码，你需要从``/input/input.json``中读取数据进行预测以得到相应摘要，该数据格式与下发数据格式完全一致，但会删除"answer"字段。选手需要将预测的结果输出到`/output/result.json`中，预测结果文件为一个json格式的文件，具体可以查看``evaluate/result.json``。

## 评测指标

本赛道采用的ROUGE(Recall-Oriented Understudy for Gisting Evaluation)评价评价。ROUGE指标将自动生成的摘要与参考摘要进行比较, 其中ROUGE-1衡量unigram匹配情况，ROUGE-2衡量bigram匹配，ROUGE-L记录最长的公共子序列。三者都只采用f-score，且总分的计算方式为：```0.2*f-score(R1)+0.4*f-score(R2)+0.4*f-score(RL)```


## 现有的系统环境


python库的环境列表： 
[详见envs](./envs)

如果你有其他环境或者python库要求，可以在issue中写明具体需求。